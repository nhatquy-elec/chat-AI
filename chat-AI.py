import os
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain

# ğŸš€ TiÃªu Ä‘á» giao diá»‡n
st.title("ğŸ“„ Chatbot tá»« tÃ i liá»‡u GitHub â€“ dÃ¹ng OpenRouter")

# âœ… Khá»Ÿi táº¡o mÃ´ hÃ¬nh nhÃºng vÄƒn báº£n (khÃ´ng dÃ¹ng API)
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# âœ… Khá»Ÿi táº¡o mÃ´ hÃ¬nh LLM tá»« OpenRouter
llm = ChatOpenAI(
    temperature=0.1,
    openai_api_key=st.secrets["openai_api_key"],
    openai_api_base="https://openrouter.ai/api/v1",
    model_name="meta-llama/llama-4-maverick:free"
)

# âœ… Náº¡p vÃ  xá»­ lÃ½ tÃ i liá»‡u
@st.cache_resource
def load_db():
    docs = []
    for filename in os.listdir("data"):
        path = os.path.join("data", filename)
        if filename.endswith(".pdf"):
            docs += PyPDFLoader(path).load()
        elif filename.endswith(".docx") or filename.endswith(".txt"):
            docs += UnstructuredFileLoader(path).load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = splitter.split_documents(docs)
    db = FAISS.from_documents(splits, embedding_model)
    return db

db = load_db()

# âœ… Giao diá»‡n Ä‘áº·t cÃ¢u há»i
query = st.text_input("â“ CÃ¢u há»i cá»§a báº¡n:")
if query:
    docs = db.similarity_search(query, k=3)
    chain = load_qa_chain(llm, chain_type="stuff")
    answer = chain.run(input_documents=docs, question=query)
    st.success(answer)
